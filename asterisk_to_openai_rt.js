debugger;
// –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥—É–ª–µ–π Node.js
const ari = require('ari-client'); // –ö–ª–∏–µ–Ω—Ç Asterisk REST Interface (ARI)
const WebSocket = require('ws'); // –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ WebSocket –¥–ª—è OpenAI real-time API
const fs = require('fs'); // –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π (–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—É–¥–∏–æ)
const dgram = require('dgram'); // –†–∞–±–æ—Ç–∞ —Å UDP (–¥–ª—è RTP –∞—É–¥–∏–æ)
const winston = require('winston'); // –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
const chalk = require('chalk'); // –¶–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
const async = require('async'); // –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ RTP)
const path = require('path');
const { parsePhoneNumberFromString } = require('libphonenumber-js');
require('dotenv').config(); // –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ .env

// –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –∑–∞–≥—Ä—É–∂–∞–µ–º—ã–µ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã –∏–ª–∏ –±–µ—Ä—É—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
const ARI_URL = 'http://127.0.0.1:8088'; // –ê–¥—Ä–µ—Å ARI (Asterisk REST Interface)
const ARI_USER = 'asterisk'; // –ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ARI
const ARI_PASS = 'asterisk'; // –ü–∞—Ä–æ–ª—å ARI
const ARI_APP = 'stasis_app'; // –ò–º—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Stasis

const OPENAI_API_KEY = process.env.OPENAI_API_KEY; // –ö–ª—é—á OpenAI –∏–∑ .env
const REALTIME_URL = 'wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2025-06-03'; // WebSocket URL –º–æ–¥–µ–ª–∏ GPT-4o real-time

const RTP_PORT = 12000; // –õ–æ–∫–∞–ª—å–Ω—ã–π –ø–æ—Ä—Ç –¥–ª—è –ø—Ä–∏—ë–º–∞ RTP –∞—É–¥–∏–æ

const MAX_CALL_DURATION = process.env.MAX_CALL_DURATION ? parseInt(process.env.MAX_CALL_DURATION) : 300000; // –ú–∞–∫—Å. –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–≤–æ–Ω–∫–∞ –≤ –º—Å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5 –º–∏–Ω—É—Ç)
const RTP_QUEUE_CONCURRENCY = parseInt(process.env.RTP_QUEUE_CONCURRENCY) || 50; // –ö–æ–ª-–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç–ø—Ä–∞–≤–æ–∫ RTP-–ø–∞–∫–µ—Ç–æ–≤
const LOG_RTP_EVERY_N_PACKETS = parseInt(process.env.LOG_RTP_EVERY_N_PACKETS) || 100; // –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å RTP-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ N –ø–∞–∫–µ—Ç–æ–≤
const ENABLE_RTP_LOGGING = process.env.ENABLE_RTP_LOGGING === 'true'; // –í–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ RTP
const ENABLE_SENT_TO_OPENAI_RECORDING = process.env.ENABLE_SENT_TO_OPENAI_RECORDING === 'true'; // –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∞—É–¥–∏–æ, –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –≤ OpenAI (.raw –∏ .wav)

const VAD_THRESHOLD = process.env.VAD_THRESHOLD ? parseFloat(process.env.VAD_THRESHOLD) : 0.1; // –ü–æ—Ä–æ–≥ VAD (—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ—á–∏)
const VAD_PREFIX_PADDING_MS = process.env.VAD_PREFIX_PADDING_MS ? parseInt(process.env.VAD_PREFIX_PADDING_MS) : 300; // –ü—Ä–µ—Ñ–∏–∫—Å–Ω—ã–π –æ—Ç—Å—Ç—É–ø VAD –≤ –º—Å (–¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–µ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–π —Ä–µ—á—å—é)
const VAD_SILENCE_DURATION_MS = process.env.VAD_SILENCE_DURATION_MS ? parseInt(process.env.VAD_SILENCE_DURATION_MS) : 500; // –í—Ä–µ–º—è —Ç–∏—à–∏–Ω—ã VAD –≤ –º—Å (–ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Ä–µ—á—å —Å—á–∏—Ç–∞–µ—Ç—Å—è –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–π)

const TARGET_RMS = 0.15; // –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ RMS –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –∞—É–¥–∏–æ
const MIN_RMS = 0.001; // –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ RMS –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —É—Å–∏–ª–µ–Ω–∏—è –∞—É–¥–∏–æ

// Counters for client/server event logging
let sentEventCounter = 0; // Tracks sent events to OpenAI
let receivedEventCounter = -1; // Tracks received events from OpenAI

// Configure Winston logger with timestamp and colorized output
const logger = winston.createLogger({
  level: 'info', // Log level
  format: winston.format.combine(
    winston.format.timestamp(), // Add timestamp to logs
    winston.format.printf(({ timestamp, level, message }) => {
      const [origin] = message.split(' ', 1); // Extract message origin (Client/Server)
      let counter;
      let coloredMessage;
      if (origin === '[Client]') {
        counter = `C-${sentEventCounter.toString().padStart(4, '0')}`; // Client event counter
        sentEventCounter++;
        coloredMessage = chalk.cyanBright(message); // Cyan for client messages
      } else if (origin === '[Server]') {
        counter = `S-${receivedEventCounter.toString().padStart(4, '0')}`; // Server event counter
        receivedEventCounter++;
        coloredMessage = chalk.yellowBright(message); // Yellow for server messages
      } else {
        counter = 'N/A'; // No counter for general logs
        coloredMessage = chalk.gray(message); // Gray for general logs
      }
      return `${counter} | ${timestamp} [${level.toUpperCase()}] ${coloredMessage}`; // Formatted log line
    })
  ),
  transports: [new winston.transports.Console()] // Output logs to console
});

// Helper functions for logging OpenAI events
const logClient = (msg) => logger.info(`[Client] ${msg}`); // Log client-side OpenAI events
const logServer = (msg) => logger.info(`[Server] ${msg}`); // Log server-side OpenAI events

// Maps to track channel states and audio buffers
const extMap = new Map(); // Maps ExternalMedia channels to their bridges and SIP channels
const sipMap = new Map(); // Maps SIP channels to their WebSocket and bridge data
const rtpSender = dgram.createSocket('udp4'); // Single UDP socket for sending RTP packets
let rtpReceiver = dgram.createSocket('udp4'); // UDP socket for receiving RTP packets
let ariClient; // ARI client instance

const audioFromAsteriskMap = new Map(); // Buffers audio received from Asterisk
const audioToOpenAIMap = new Map(); // Buffers audio sent to OpenAI
const amplificationLogFrequency = new Map(); // Tracks last amplification log time per channel
const rmsLogFrequency = new Map(); // Tracks last RMS log time per channel
const rtpSentStats = new Map(); // Tracks RTP stats per channel

// –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–∏ validateRussianPhone
function validateRussianPhone(raw) {
  // —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –¥–µ—Ñ–∏—Å—ã –∏ —Å–∫–æ–±–∫–∏, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –ø–æ—è–≤–∏–ª–∏—Å—å
  const cleaned = String(raw).replace(/[^\d+]/g, '');
  try {
    const pn = parsePhoneNumberFromString(cleaned, 'RU');
    // –≤–∞–ª–∏–¥–Ω—ã–π –ª–∏ –Ω–æ–º–µ—Ä –∏ —Ç–æ—á–Ω–æ –ª–∏ –æ–Ω —Ä–æ—Å—Å–∏–π—Å–∫–∏–π
    if (pn?.isValid() && pn.country === 'RU') {
      return pn.number;            // –≤–µ—Ä–Ω—ë—Ç —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ +7XXXXXXXXXX
    }
  } catch (_) { /* ignore */ }
  return null;                      // –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π
}

async function handleValidatePhone(call, ws, logger) {
  if (!call.arguments) {
    logger.error('validate_phone: arguments missing');
    return;
  }

  let args;
  try {
    args = typeof call.arguments === 'string'
      ? JSON.parse(call.arguments)
      : call.arguments;
  } catch (e) {
    logger.error('validate_phone: bad JSON:', e);
    return;
  }

  const phone = String(args.phone);
  logger.info(`üîç [PHONE] –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–∞ —á–µ—Ä–µ–∑ tools: "${phone}"`);

  const formattedPhone = validateRussianPhone(phone);

  if (!formattedPhone) {
    logger.warn(`[PHONE] –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω: ${phone}`);
    if (ws && ws.readyState === ws.OPEN) {
      ws.send(JSON.stringify({
        type: 'response.create',
        response: {
          modalities: ['audio', 'text'],
          instructions: `–°–∫–∞–∂–∏ —Ä–æ–≤–Ω–æ: "–ü–æ—Ö–æ–∂–µ, –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –Ω–æ–º–µ—Ä –ø–æ–ª–Ω–æ—Å—Ç—å—é, –Ω–∞—á–∏–Ω–∞—è —Å +7."`,
          temperature: 0.6
        }
      }));
    }
  } else {
    logger.info(`[PHONE] –í–∞–ª–∏–¥–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω: ${formattedPhone}`);
    if (ws && ws.readyState === ws.OPEN) {
      ws.send(JSON.stringify({
        type: 'response.create',
        response: {
          modalities: ['audio', 'text'],
          instructions: `–°–∫–∞–∂–∏ —Ä–æ–≤–Ω–æ: "–Ø –∑–∞–ø–∏—Å–∞–ª–∞ –Ω–æ–º–µ—Ä ${formattedPhone}. –í—Å—ë –≤–µ—Ä–Ω–æ?`,
          temperature: 0.6
        }
      }));
    }
  }
}
// –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–∏ save_client_info
const { spawn } = require('child_process');

/**
 * –ó–∞–ø—É—Å–∫–∞–µ—Ç save_client_info.py –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –µ–≥–æ –≤—ã–≤–æ–¥.
 * –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Å—Ç—Ä–æ–∫–∏ ¬´‚úÖ –ù–æ–º–µ—Ä –Ω–æ–≤–æ–π –∑–∞—è–≤–∫–∏: <num>¬ª –≤–µ—Ä–Ω—ë—Ç orderNumber.
 */
async function runSaveClientInfo(clientData, logger) {
  return new Promise((resolve, reject) => {
    const proc = spawn('python3', ['-u', 'save_client_info.py'], { stdio: ['pipe', 'pipe', 'pipe'] });
    let orderNumber = null;

    proc.stdout.on('data', buf => {
      buf.toString().split(/\r?\n/).filter(Boolean).forEach(line => {
        logger.info(`[save_client_info] ${line}`);
        const m = line.match(/–ù–æ–º–µ—Ä –Ω–æ–≤–æ–π –∑–∞—è–≤–∫–∏:\s*([^\s]+)/);
        if (m) orderNumber = m[1];
      });
    });

    proc.stderr.on('data', buf =>
      buf.toString().split(/\r?\n/).filter(Boolean)
        .forEach(line => logger.error(`[save_client_info:stderr] ${line}`))
    );

    proc.on('close', code => {
      if (code === 0 && orderNumber) return resolve(orderNumber);
      const msg = `save_client_info.py exited with code ${code}`;
      logger.error(msg);
      reject(new Error(msg));
    });

    proc.stdin.write(JSON.stringify(clientData));
    proc.stdin.end();
  });
}


// --- –æ—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ ----------------------------------------------------
async function handleSaveClientInfo(call, ws, logger) {
  /* ---------- 1. parse arguments ----------------------------------------- */
  if (!call.arguments) return logger.error('save_client_info: arguments missing');

  let args;
  try {
    args = typeof call.arguments === 'string'
      ? JSON.parse(call.arguments)
      : call.arguments;
  } catch (e) {
    return logger.error('save_client_info: bad JSON:', e);
  }

  /* ---------- 2. build payload for Python -------------------------------- */
  const channelEntry = Array.from(sipMap.entries()).find(([, data]) => data.ws === ws);
  const callerNumber = channelEntry ? channelEntry[1].callerNumber : null;

  const clientData = {
    name:  args.name,
    direction: args.direction,
    circumstances: args.circumstances || '',
    brand: args.brand || '',
    phone: String(args.phone),
    phone2: callerNumber || '',
    address: {
      city: args.address?.city,
      street: args.address?.street,
      house_number: args.address?.house_number,
      apartment: args.address?.apartment || '',
      entrance: args.address?.entrance || '',
      floor: args.address?.floor || '',
      intercom: args.address?.intercom || '',
      latitude: args.address?.latitude,
      longitude: args.address?.longitude
    },
    date: args.date || '',
    comment: args.comment || ''
  };

  /* ---------- 3. run Python ---------------------------------------------- */
  let orderNum;
  try {
    orderNum = await runSaveClientInfo(clientData, logger); // ‚Üê –ª–æ–≤–∏—Ç ¬´–ü–ª2251279¬ª
    logger.info(`–ó–∞—è–≤–∫–∞ —Å–æ–∑–¥–∞–Ω–∞, –Ω–æ–º–µ—Ä ${orderNum}`);
  } catch (err) {
    logger.error(`save_client_info: ${err.message}`);

    // –≤–µ–∂–ª–∏–≤–æ —Å–æ–æ–±—â–∞–µ–º –æ–± –æ—à–∏–±–∫–µ
    if (ws?.readyState === ws.OPEN) {
      ws.send(JSON.stringify({
        type: 'response.create',
        response: {
          modalities: ['audio', 'text'],
          instructions: '–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –∑–∞—è–≤–∫—É —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–µ —É–¥–∞–ª–æ—Å—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.'
        }
      }));
    }
    return;
  }

  /* ---------- 4. tell the user the ticket number ------------------------- */
  if (ws && ws.readyState === ws.OPEN) {
    const reply = `–í–∞—à–∞ –∑–∞—è–≤–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞. –ù–æ–º–µ—Ä ${orderNum}. –°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—Ä–∞—â–µ–Ω–∏–µ!`;

    ws.send(
      JSON.stringify({
        type: 'response.create',
        response: {
          modalities: ['audio', 'text'],
          instructions: `–°–∫–∞–∂–∏ —Ä–æ–≤–Ω–æ –∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ–∑–≤—É—á—å –Ω–æ–º–µ—Ä –∑–∞—è–≤–∫–∏: "${reply}"`, // üîí —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É
          temperature: 0.6
        }
      })
    );

    logger.info(`üîî [Client] –û—Ç–≤–µ—Ç —Å –Ω–æ–º–µ—Ä–æ–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ OpenAI: ${orderNum}`);
  }}


// Add an ExternalMedia channel to a bridge with retry logic
async function addExtToBridge(client, channel, bridgeId, retries = 5, delay = 500) {
  try {
    const bridge = await client.bridges.get({ bridgeId }); // Fetch bridge by ID
    if (!bridge) throw new Error('Bridge not found');
    await bridge.addChannel({ channel: channel.id }); // Add channel to bridge
    logger.info(`ExternalMedia channel ${channel.id} added to bridge ${bridgeId}`);
  } catch (err) {
    if (retries) {
      logger.info(`Retrying to add ExternalMedia channel ${channel.id} to bridge ${bridgeId} (${retries} attempts remaining)`);
      await new Promise(r => setTimeout(r, delay)); // Wait before retrying
      return addExtToBridge(client, channel, bridgeId, retries - 1, delay); // Recursive retry
    }
    logger.error(`Error adding ExternalMedia channel ${channel.id} to bridge ${bridgeId}: ${err.message}`);
  }
}

// Start the RTP receiver to listen for audio from Asterisk
function startRTPReceiver() {
  let packetCount = 0; // Count of received RTP packets
  let totalBytes = 0; // Total bytes received
  let startTime = Date.now(); // Start time for rate calculation
  const audioBuffers = new Map(); // Temporary audio buffers per channel
  const BUFFER_INTERVAL_MS = 200; // Interval to process audio chunks (ms)

  rtpReceiver.on('listening', () => {
    const address = rtpReceiver.address();
    logger.info(`RTP Receiver listening on ${address.address}:${address.port}`);
  });

  // Handle incoming RTP packets
  rtpReceiver.on('message', (msg, rinfo) => {
    packetCount++;
    totalBytes += msg.length;
    if (packetCount >= 100) { // Log stats every 100 packets
      const currentTime = Date.now();
      const duration = (currentTime - startTime) / 1000;
      const rate = (packetCount / duration).toFixed(2);
      logger.info(`Received ${packetCount} RTP packets from ${rinfo.address}:${rinfo.port}, total bytes: ${totalBytes}, rate: ${rate} packets/s`);
      packetCount = 0;
      totalBytes = 0;
      startTime = currentTime;
    }

    // Find channel ID based on RTP source
    const channelId = [...sipMap.entries()].find(([_, data]) => data.rtpSource && data.rtpSource.address === rinfo.address && data.rtpSource.port === rinfo.port)?.[0];
    if (channelId) {
      const muLawData = msg.slice(12); // Extract Œº-law payload (skip RTP header)
      if (!audioFromAsteriskMap.has(channelId)) audioFromAsteriskMap.set(channelId, Buffer.alloc(0));
      audioFromAsteriskMap.set(channelId, Buffer.concat([audioFromAsteriskMap.get(channelId), muLawData])); // Append to Asterisk audio buffer

      const pcmBuffer24kHz = muLawToPcm24kHz(muLawData, channelId); // Convert to PCM 24kHz
      if (!audioBuffers.has(channelId)) audioBuffers.set(channelId, Buffer.alloc(0));
      audioBuffers.set(channelId, Buffer.concat([audioBuffers.get(channelId), pcmBuffer24kHz])); // Append to temporary buffer

      // Set up interval to send audio to OpenAI
      if (!sipMap.get(channelId).sendTimeout) {
        sipMap.get(channelId).sendTimeout = setInterval(() => {
          const buffer = audioBuffers.get(channelId);
          if (buffer && buffer.length > 0) {
            let sumSquares = 0;
            for (let i = 0; i < buffer.length / 2; i++) { // Calculate RMS
              const sample = buffer.readInt16LE(i * 2);
              sumSquares += sample * sample;
            }
            const rms = Math.sqrt(sumSquares / (buffer.length / 2)) / 32768;
            const now = Date.now();
            if (rms < TARGET_RMS && rms > MIN_RMS) { // Normalize audio if RMS is low
              const gain = Math.min(TARGET_RMS / rms, 2);
              for (let i = 0; i < buffer.length / 2; i++) {
                let sample = buffer.readInt16LE(i * 2);
                sample = Math.round(sample * gain);
                sample = Math.max(-32768, Math.min(32767, sample));
                buffer.writeInt16LE(sample, i * 2);
              }
              if (!rmsLogFrequency.has(channelId) || now - rmsLogFrequency.get(channelId) >= 2000) {
                logger.info(`Adjusted RMS from ${rms.toFixed(3)} to ~${TARGET_RMS} with gain ${gain.toFixed(2)} for channel ${channelId}`);
                rmsLogFrequency.set(channelId, now);
              }
            }

            const base64Audio = buffer.toString('base64'); // Convert to base64 for OpenAI
            const channelData = sipMap.get(channelId);
            if (channelData && channelData.ws && channelData.ws.readyState === WebSocket.OPEN) {
              channelData.ws.send(JSON.stringify({ type: 'input_audio_buffer.append', audio: base64Audio })); // Send to OpenAI
              if (!rmsLogFrequency.has(channelId) || now - rmsLogFrequency.get(channelId) >= 2000) {
                logClient(`Sending audio chunk to OpenAI for channel ${channelId} | Size: ${(buffer.length / 1024).toFixed(2)} KB | RMS: ${rms.toFixed(3)}`);
                rmsLogFrequency.set(channelId, now);
              }
            }
            audioBuffers.set(channelId, Buffer.alloc(0)); // Clear buffer after sending
          }
        }, BUFFER_INTERVAL_MS);
      }
    }
  });

  rtpReceiver.on('error', (err) => {
    logger.error(`RTP Receiver error: ${err.message}`);
  });

  rtpReceiver.bind(RTP_PORT, '127.0.0.1'); // Bind to local port
}

// Convert a single Œº-law sample to 16-bit PCM
function muLawToPcm16(muLaw) {
  muLaw = ~muLaw & 0xFF; // Invert bits and mask
  const sign = (muLaw & 0x80) ? -1 : 1; // Extract sign
  const exponent = (muLaw & 0x70) >> 4; // Extract exponent
  const mantissa = muLaw & 0x0F; // Extract mantissa
  let sample = (exponent === 0) ? (mantissa * 8 + 16) : (1 << (exponent + 3)) * (mantissa + 16) - 128; // Decode sample
  sample = sign * sample;
  return Math.max(-32768, Math.min(32767, sample)); // Clamp to 16-bit range
}

// Convert Œº-law buffer to 24kHz PCM with interpolation
function muLawToPcm24kHz(muLawBuffer, channelId) {
  const pcm8kHz = Buffer.alloc(muLawBuffer.length * 2); // Buffer for 8kHz PCM
  let maxSampleBefore = 0; // Track max sample before clamping
  let maxSampleAfter = 0; // Track max sample after clamping

  // Convert Œº-law to 8kHz PCM
  for (let i = 0; i < muLawBuffer.length; i++) {
    let sample = muLawToPcm16(muLawBuffer[i]);
    maxSampleBefore = Math.max(maxSampleBefore, Math.abs(sample));
    sample = Math.round(sample);
    sample = Math.max(-32768, Math.min(32767, sample));
    maxSampleAfter = Math.max(maxSampleAfter, Math.abs(sample));
    pcm8kHz.writeInt16LE(sample, i * 2);
  }

  // Upsample to 24kHz with linear interpolation
  const pcm24kHz = Buffer.alloc(muLawBuffer.length * 3 * 2);
  let sumSquares = 0;
  for (let i = 0; i < muLawBuffer.length; i++) {
    const sample = pcm8kHz.readInt16LE(i * 2);
    const prevSample = i > 0 ? pcm8kHz.readInt16LE((i - 1) * 2) : sample;
    const nextSample = i < muLawBuffer.length - 1 ? pcm8kHz.readInt16LE((i + 1) * 2) : sample;
    const interp1 = Math.round((prevSample * 0.5 + sample * 0.5)); // First interpolated sample
    const interp2 = Math.round((sample * 0.75 + nextSample * 0.25)); // Second interpolated sample
    pcm24kHz.writeInt16LE(prevSample, (i * 3) * 2);
    pcm24kHz.writeInt16LE(interp1, (i * 3 + 1) * 2);
    pcm24kHz.writeInt16LE(interp2, (i * 3 + 2) * 2);
    sumSquares += prevSample * prevSample + interp1 * interp1 + interp2 * interp2; // For RMS calculation
  }

  const rms = Math.sqrt(sumSquares / (muLawBuffer.length * 3)) / 32768; // Calculate RMS
  if (!audioToOpenAIMap.has(channelId)) audioToOpenAIMap.set(channelId, Buffer.alloc(0));
  audioToOpenAIMap.set(channelId, Buffer.concat([audioToOpenAIMap.get(channelId), pcm24kHz])); // Append to OpenAI buffer

  const now = Date.now();
  if (!amplificationLogFrequency.has(channelId) || now - amplificationLogFrequency.get(channelId) >= 2000) {
    logger.info(`Audio processed for channel ${channelId} | RMS: ${rms.toFixed(3)} | Max sample before: ${maxSampleBefore}, after: ${maxSampleAfter}`);
    amplificationLogFrequency.set(channelId, now); // Update log frequency
  }

  return pcm24kHz;
}

// Save PCM data as a WAV file
function saveWavFile(pcmData, filename, sampleRate) {
  const bitsPerSample = 16; // 16-bit audio
  const channels = 1; // Mono
  const byteRate = sampleRate * channels * (bitsPerSample / 8);
  const blockAlign = channels * (bitsPerSample / 8);
  const dataSize = pcmData.length;
  const fileSize = 36 + dataSize;

  const buffer = Buffer.alloc(44 + dataSize); // WAV header + data
  buffer.write('RIFF', 0);
  buffer.writeUInt32LE(fileSize, 4);
  buffer.write('WAVE', 8);
  buffer.write('fmt ', 12);
  buffer.writeUInt32LE(16, 16); // Subchunk size
  buffer.writeUInt16LE(1, 20); // PCM format
  buffer.writeUInt16LE(channels, 22);
  buffer.writeUInt32LE(sampleRate, 24);
  buffer.writeUInt32LE(byteRate, 28);
  buffer.writeUInt16LE(blockAlign, 32);
  buffer.writeUInt16LE(bitsPerSample, 34);
  buffer.write('data', 36);
  buffer.writeUInt32LE(dataSize, 40);
  pcmData.copy(buffer, 44); // Copy PCM data

  fs.writeFileSync(filename, buffer);
  logger.info(`Saved audio as ${filename}`);
}

// Save raw Œº-law data to a file
function saveRawFile(data, filename) {
  fs.writeFileSync(filename, data);
  logger.info(`Saved raw Œº-law as ${filename}`);
}

// Convert 16-bit PCM sample to Œº-law
function pcm16ToMuLaw(sample) {
  const MAX = 32767;
  const MU = 255;
  const BIAS = 33;

  sample = Math.max(-MAX, Math.min(MAX, sample)); // Clamp to 16-bit range
  const sign = sample < 0 ? 0x80 : 0;
  let absSample = Math.abs(sample);

  if (absSample < 50) return 0x7F; // Silence threshold
  absSample += BIAS;

  const normalized = absSample / MAX;
  const muLaw = Math.log(1 + MU * normalized) / Math.log(1 + MU); // Œº-law compression
  const quantized = Math.round(muLaw * 128);
  const exponent = Math.min(Math.floor(quantized / 16), 7);
  const mantissa = Math.min((quantized - (exponent * 16)), 15) & 0x0F;

  return ~(sign | (exponent << 4) | mantissa) & 0xFF; // Invert bits
}

// Resample 24kHz PCM to 8kHz
function resamplePcm24kHzTo8kHz(pcm24kHz) {
  const inSampleRate = 24000;
  const outSampleRate = 8000;
  const inSamples = pcm24kHz.length / 2;
  const outSamples = Math.floor(inSamples * outSampleRate / inSampleRate);
  const pcm8kHz = Buffer.alloc(outSamples * 2);

  for (let i = 0; i < outSamples; i++) {
    const srcPos = i * inSampleRate / outSampleRate;
    const srcIndex = Math.floor(srcPos);
    const frac = srcPos - srcIndex;

    if (srcIndex + 1 < inSamples) {
      const sample1 = pcm24kHz.readInt16LE(srcIndex * 2);
      const sample2 = pcm24kHz.readInt16LE((srcIndex + 1) * 2);
      const interpSample = Math.round(sample1 + frac * (sample2 - sample1)); // Linear interpolation
      pcm8kHz.writeInt16LE(interpSample, i * 2);
    } else if (srcIndex < inSamples) {
      pcm8kHz.writeInt16LE(pcm24kHz.readInt16LE(srcIndex * 2), i * 2);
    }
  }
  return pcm8kHz;
}

// Convert PCM buffer to Œº-law, optionally resampling from 24kHz to 8kHz
function pcmToMuLaw(pcmBuffer, resample = false) {
  const input = resample ? resamplePcm24kHzTo8kHz(pcmBuffer) : pcmBuffer;
  const muLawBuffer = Buffer.alloc(input.length / 2);
  const chunkSize = 1024;
  for (let i = 0; i < input.length / 2; i += chunkSize) {
    const end = Math.min(i + chunkSize, input.length / 2);
    for (let j = i; j < end; j++) {
      let sample = input.readInt16LE(j * 2);
      sample = Math.max(-32767, Math.min(32767, Math.floor(sample * 0.95))); // Apply slight attenuation
      muLawBuffer[j] = pcm16ToMuLaw(sample);
    }
  }
  return muLawBuffer;
}

// Build RTP header for a packet
function buildRTPHeader(seq, timestamp, ssrc) {
  const header = Buffer.alloc(12);
  header[0] = 0x80; // Version 2, no padding, no extension
  header[1] = 0x00; // Payload type (0 for Œº-law)
  header.writeUInt16BE(seq, 2); // Sequence number
  header.writeUInt32BE(timestamp, 4); // Timestamp
  header.writeUInt32BE(ssrc, 8); // Synchronization source
  return header;
}

// Async queue for sending RTP packets
const rtpQueue = async.queue((task, callback) => {
  rtpSender.send(task.packet, task.port, task.address, callback);
}, RTP_QUEUE_CONCURRENCY);

// Send an RTP packet with Œº-law data
async function sendAudioPacket(muLawData, port, address, seq, timestamp, ssrc) {
  const startTime = process.hrtime.bigint();
  const header = buildRTPHeader(seq, timestamp, ssrc);
  const rtpPacket = Buffer.concat([header, muLawData]);
  await new Promise((resolve, reject) => {
    rtpQueue.push({ packet: rtpPacket, port, address }, (err) => {
      const elapsedMs = Number(process.hrtime.bigint() - startTime) / 1e6;
      if (ENABLE_RTP_LOGGING && seq % LOG_RTP_EVERY_N_PACKETS === 0) {
        logger.info(`Sent packet seq=${seq}, timestamp=${timestamp}, elapsed=${elapsedMs.toFixed(2)}ms`);
      }
      if (err) {
        logger.error(`Error sending RTP packet: ${err.message}`);
        reject(err);
      } else {
        resolve();
      }
    });
  });
}

// Stream audio to Asterisk via RTP
const MAX_BUFFER_SIZE = 1024 * 1024; // Max buffer size (1MB)
async function streamAudio(channelId, rtpSource, initialBuffer = Buffer.alloc(0)) {
  const samplesPerPacket = 80; // 10 ms at 8000 Hz
  const packetIntervalNs = BigInt(10 * 1e6); // 10 ms in nanoseconds
  const { address, port } = rtpSource;

  logger.info(`Initializing RTP stream to ${address}:${port} for channel ${channelId}`);

  let rtpSequence = Math.floor(Math.random() * 65535); // Random initial sequence
  let rtpTimestamp = 0; // Initial timestamp
  const rtpSSRC = Math.floor(Math.random() * 4294967295); // Random SSRC
  let streamStartTime = process.hrtime.bigint();
  let isStreaming = true;
  let totalBytesSent = 0;
  let totalPacketsSent = 0;
  let stopRequested = false;
  let lastBufferSize = 0; // Previous buffer size
  let wasSending = false; // Track if we were sending data

  let muLawBuffer = Buffer.alloc(0); // Buffer for Œº-law data
  let offset = 0; // Offset in buffer

  if (!rtpSentStats.has(channelId)) {
    rtpSentStats.set(channelId, { packets: 0, bytes: 0, startTime: null }); // Initialize stats
  }

  // Send a batch of RTP packets
  const sendPackets = async (data, packetCount, isSilence = false) => {
    let blockStartTime = process.hrtime.bigint();
    let nextPacketTime = blockStartTime;

    for (let i = 0; i < packetCount && !stopRequested; i++) {
      const bytesToSend = Math.min(samplesPerPacket, data.length - (i * samplesPerPacket));
      const packetData = data.slice(i * samplesPerPacket, i * samplesPerPacket + bytesToSend);
      const packetDataPadded = bytesToSend < samplesPerPacket ? Buffer.concat([packetData, Buffer.alloc(samplesPerPacket - bytesToSend, 0x7F)]) : packetData;

      await sendAudioPacket(packetDataPadded, port, address, rtpSequence, rtpTimestamp, rtpSSRC);
      if (i === 0 && !streamStartTime) streamStartTime = process.hrtime.bigint();
      rtpSequence = (rtpSequence + 1) % 65536;
      rtpTimestamp += 80;
      totalBytesSent += packetDataPadded.length;
      totalPacketsSent += 1;

      const stats = rtpSentStats.get(channelId);
      stats.packets += 1;
      stats.bytes += packetDataPadded.length;
      if (!stats.startTime) stats.startTime = Date.now();

      nextPacketTime += packetIntervalNs;
      const now = process.hrtime.bigint();
      if (now < nextPacketTime) {
        const delayMs = Number(nextPacketTime - now) / 1e6;
        await new Promise(resolve => setTimeout(resolve, delayMs)); // Maintain timing
      }
    }
  };

  const silencePacket = Buffer.alloc(samplesPerPacket, 0x7F); // Silence packet
  await sendPackets(silencePacket, 10, true); // Send initial silence
  logger.info(`RTP stream fully initialized for channel ${channelId}`);

  // Process PCM chunks into Œº-law
  const processFallback = async (pcmChunk) => {
    const muLawData = pcmToMuLaw(pcmChunk, true);
    muLawBuffer = Buffer.concat([muLawBuffer, muLawData]);
    if (muLawBuffer.length > MAX_BUFFER_SIZE) {
      muLawBuffer = muLawBuffer.slice(muLawBuffer.length - MAX_BUFFER_SIZE); // Trim buffer
    }
  };

  // Main streaming loop
  const streamLoop = async () => {
    while (isStreaming && !stopRequested) {
      if (!sipMap.has(channelId)) {
        logger.info(`Channel ${channelId} no longer active, stopping RTP stream`);
        break;
      }
      const currentBufferSize = muLawBuffer.length - offset;
      if (currentBufferSize >= samplesPerPacket) {
        const packetCount = Math.floor(currentBufferSize / samplesPerPacket);
        await sendPackets(muLawBuffer.slice(offset, offset + packetCount * samplesPerPacket), packetCount);
        offset += packetCount * samplesPerPacket;
        if (muLawBuffer.length - offset > MAX_BUFFER_SIZE / 2) {
          muLawBuffer = muLawBuffer.slice(offset);
          offset = 0; // Reset offset
        }
        wasSending = true;
      } else if (wasSending && currentBufferSize < samplesPerPacket) {
        logger.info(`RTP buffer to Asterisk fully sent for channel ${channelId} | Remaining: ${currentBufferSize} bytes`);
        wasSending = false;
      }
      lastBufferSize = currentBufferSize;
      await new Promise(resolve => setImmediate(resolve)); // Yield control
    }

    const totalDuration = Number(process.hrtime.bigint() - streamStartTime) / 1e9;
    logger.info(`Finished RTP stream for channel ${channelId} | Total duration: ${totalDuration.toFixed(2)}s | Total bytes sent: ${totalBytesSent} | Total packets: ${totalPacketsSent}`);
    rtpSentStats.set(channelId, { packets: 0, bytes: 0, startTime: null }); // Reset stats
  };

  streamLoop();

  // Stop the stream
  const stop = async () => {
    isStreaming = false;
    stopRequested = true;
    muLawBuffer = Buffer.alloc(0);
    offset = 0;
    logger.info(`RTP stream stopped for channel ${channelId}`);
  };

  return {
    stop,
    write: processFallback, // Method to write PCM data
    muLawBuffer,
    offset
  };
}

// Start WebSocket connection to OpenAI real-time API
function startOpenAIWebSocket(channelId) {
  logger.info(`Attempting to start OpenAI WebSocket for channel ${channelId}`);
  const ws = new WebSocket(REALTIME_URL, {
    headers: { 'Authorization': `Bearer ${OPENAI_API_KEY}`, 'OpenAI-Beta': 'realtime=v1' } // Authentication headers
  });

  let responseTimestamp = null; // Timestamp of response start
  let responseTranscript = ''; // Accumulated transcript
  let audioDeltaCount = 0; // Count of audio fragments
  let transcriptDeltaCount = 0; // Count of transcript fragments
  let audioReceivedLogged = false; // Flag for first audio log
  let audioSentTime = null; // Time audio was sent to OpenAI
  let callStartTime = null; // Call start time
  let maxCallTimeoutId = null; // Timeout ID for max call duration
  let totalPacketsSentThisResponse = 0; // Packets sent for current response
  let totalPacketsSentSession = 0; // Total packets sent in session
  let playbackComplete = false; // Playback completion flag
  let streamHandler = null; // RTP stream handler
  let isPlayingResponse = false; // Flag for active response playback

  // Initialize RTP stream handler
  const initializeStreamHandler = async () => {
    const channelData = sipMap.get(channelId);
    if (channelData && channelData.rtpSource) {
      streamHandler = await streamAudio(channelId, channelData.rtpSource);
      logger.info(`StreamHandler initialized for channel ${channelId} | Ready: ${streamHandler !== null}`);
    } else {
      logger.error(`Cannot initialize StreamHandler: No RTP source for channel ${channelId}`);
    }
    return streamHandler;
  };
// 1. –ó–∞—Ä–∞–Ω–µ–µ –æ–±—ä—è–≤–∏—Ç–µ –º–∞—Å—Å–∏–≤ (–ª—É—á—à–µ –≤–≤–µ—Ä—Ö—É —Ñ–∞–π–ª–∞, –Ω–æ –º–æ–∂–Ω–æ –ø—Ä—è–º–æ –∑–¥–µ—Å—å)
const tools = [
  {
    type: 'function',
    name: 'save_client_info',
    description: '–°–æ–∑–¥–∞—ë—Ç –∑–∞—è–≤–∫—É –∫–ª–∏–µ–Ω—Ç–∞ –≤ 1–° –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ',
    parameters: {
      type: 'object',
      required: ['name', 'direction', 'phone', 'address'],
      properties: {
        name:        { type: 'string',  description: '–ò–º—è –∫–ª–∏–µ–Ω—Ç–∞' },
        direction:   { type: 'string',  description: '—Ü–µ–ª—å / –ø—Ä–∏—á–∏–Ω–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è',
          enum: [
            '–•–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫–∏',
            '–ö–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä—ã',
            '–¢–µ–ª–µ–≤–∏–∑–æ—Ä—ã',
            '–°—Ç–∏—Ä–∞–ª—å–Ω—ã–µ –º–∞—à–∏–Ω—ã',
            '–ü–æ—Å—É–¥–æ–º–æ–µ—á–Ω—ã–µ –º–∞—à–∏–Ω—ã',
            '–®–≤–µ–π–Ω—ã–µ –º–∞—à–∏–Ω—ã',
            '–ö–æ—Ñ–µ–º–∞—à–∏–Ω—ã',
            '–ü–ª–∏—Ç—ã',
            '–ú–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–∫–∏',
            '–í—ã—Ç—è–∂–∫–∏',
            '–ö–æ–º–ø—å—é—Ç–µ—Ä—ã',
            '–ì–∞–¥–∂–µ—Ç—ã',
            '–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π —Ö–æ–ª–æ–¥',
            '–ì–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏',
            '–£—Å—Ç–∞–Ω–æ–≤–∫–∞',
            '–ü—ã–ª–µ—Å–æ—Å—ã',
            '–ö–ª–∏–Ω–∏–Ω–≥',
            '–î–µ–∑–∏–Ω—Å–µ–∫—Ü–∏—è',
            '–ù–∞—Ç—è–∂–Ω—ã–µ –ø–æ—Ç–æ–ª–∫–∏',
            '–ú–µ–ª–∫–æ–±—ã—Ç–æ–≤–æ–π —Å–µ—Ä–≤–∏—Å',
            '–†–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä',
            '–°–∞–Ω—Ç–µ—Ö–Ω–∏–∫–∞',
            '–í—ã–≤–æ–∑ –º—É—Å–æ—Ä–∞',
            '–£–±–æ—Ä–∫–∞',
            '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞',
            '–û–∫–Ω–∞'
          ]
        },
        circumstances:{ type: 'string', description: '–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–∏ / –æ–±—Ä–∞—â–µ–Ω–∏—è' },
        brand:       { type: 'string',  description: '–ë—Ä–µ–Ω–¥ –∏ –º–æ–¥–µ–ª—å —Ç–µ—Ö–Ω–∏–∫–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π' },
        phone: {
  type: 'string',
  description: '–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ +7XXXXXXXXXX',
  pattern: '^\\+7\\d{10}$'
},
        address: {
          type: 'object',
          description: '–ê–¥—Ä–µ—Å –≤—ã–µ–∑–¥–∞ –º–∞—Å—Ç–µ—Ä–∞',
          required: ['city', 'street', 'house_number'],
          properties: {
            city:        { type: 'string', description: '–ì–æ—Ä–æ–¥' },
            street:      { type: 'string', description: '–£–ª–∏—Ü–∞' },
            house_number:{ type: 'string', description: '–î–æ–º / –∫–æ—Ä–ø—É—Å / —Å—Ç—Ä–æ–µ–Ω–∏–µ' },
            apartment:   { type: 'string', description: '–ö–≤–∞—Ä—Ç–∏—Ä–∞' },
            entrance:    { type: 'string', description: '–ü–æ–¥—ä–µ–∑–¥' },
            floor:       { type: 'string', description: '–≠—Ç–∞–∂' },
            intercom:    { type: 'string', description: '–ö–æ–¥ –¥–æ–º–æ—Ñ–æ–Ω–∞' },
            latitude:    { type: 'number', description: '–®–∏—Ä–æ—Ç–∞' },
            longitude:   { type: 'number', description: '–î–æ–ª–≥–æ—Ç–∞' }
          }
        },
        date:   { type: 'string', description: '–ñ–µ–ª–∞–µ–º–∞—è –¥–∞—Ç–∞ –≤–∏–∑–∏—Ç–∞ (YYYY-MM-DD)' },
        comment:{ type: 'string', description: '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π' }
      }
    }
  },
    {
    type: 'function',
    name: 'validate_phone',
    description: '–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞.',
    parameters: {
      type: 'object',
      required: ['phone'],
      properties: {
        phone: {
          type: 'string',
          description: '–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–∏–∑–Ω—ë—Å –∫–ª–∏–µ–Ω—Ç.',
        }
      }
    }
  }
];

// WebSocket open event
  ws.on('open', async () => {
    callStartTime = Date.now();
    logClient(`OpenAI WebSocket connection established for channel ${channelId}`);
    await initializeStreamHandler();
    ws.send(JSON.stringify({
      type: 'session.update',
      session: {
        modalities: ['audio', 'text'], // –í–∫–ª—é—á–∏—Ç—å –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã
        voice: 'alloy', // –ì–æ–ª–æ—Å –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ OpenAI
        instructions:`–û—Ç–≤–µ—á–∞–π –≤—Å–µ–≥–¥–∞ –≥–æ–ª–æ—Å–æ–º –Ω–∞ –ª—é–±—É—é —Ä–µ—á—å. –¢—ã –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –ê–π—Å–±–µ—Ä–≥, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Ä–µ–º–æ–Ω—Ç–æ–º –±—ã—Ç–æ–≤–æ–π —Ç–µ—Ö–Ω–∏–∫–∏. –¢–≤–æ—è –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–∏–Ω—è—Ç—å –∑–∞—è–≤–∫—É –Ω–∞ —Ä–µ–º–æ–Ω—Ç, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Å–æ–±—Ä–∞–≤ —É –∫–ª–∏–µ–Ω—Ç–∞ —Å–ª–µ–¥—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ:
1.	–ò–º—è –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ.
2.	–¶–µ–ª—å –∏–ª–∏ –ø—Ä–∏—á–∏–Ω—É –æ–±—Ä–∞—â–µ–Ω–∏—è ‚Äî —É—Ç–æ—á–Ω—è–π, –µ—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –Ω–µ –≥–æ–≤–æ—Ä–∏—Ç —è—Å–Ω–æ –∏–ª–∏ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ.
3.	–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–∏ –∏–ª–∏ –æ–±—Ä–∞—â–µ–Ω–∏—è ‚Äî –ø–æ–ø—Ä–æ—Å–∏ –∫–ª–∏–µ–Ω—Ç–∞ –æ–ø–∏—Å–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—É, –µ—Å–ª–∏ –æ–Ω –Ω–µ –¥–∞–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
4. –ë—Ä–µ–Ω–¥ –∏ –º–æ–¥–µ–ª—å —Ç–µ—Ö–Ω–∏–∫–∏ ‚Äî —Å–ø—Ä–∞—à–∏–≤–∞–π, –µ—Å–ª–∏ —Ä–µ—á—å –∏–¥–µ—Ç –æ —Ä–µ–º–æ–Ω—Ç–µ —Ç–µ—Ö–Ω–∏–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´LG GA-B509CQSL¬ª). –ï—Å–ª–∏ —Ç–µ—Ö–Ω–∏–∫–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–π —ç—Ç–æ—Ç –ø—É–Ω–∫—Ç.
5. –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω –ª–∏—Ü–∞ –Ω–∞ –º–µ—Å—Ç–µ —Ä–µ–º–æ–Ω—Ç–∞ ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ. 
–ü–æ–ø—Ä–æ—Å–∏ –ø—Ä–æ–¥–∏–∫—Ç–æ–≤–∞—Ç—å –Ω–æ–º–µ—Ä –ø–æ–ª–Ω–æ—Å—Ç—å—é, –Ω–∞—á–∏–Ω–∞—è —Å +7. 
–ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –∫–ª–∏–µ–Ω—Ç –ø—Ä–æ–¥–∏–∫—Ç—É–µ—Ç –Ω–æ–º–µ—Ä, –í–°–ï–ì–î–ê –≤—ã–∑–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏—é validate_phone —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º. 
–ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–æ–º–µ—Ä –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ‚Äî —Å–∫–∞–∂–∏: "–ü–æ—Ö–æ–∂–µ, –Ω–æ–º–µ—Ä –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–æ–º–µ—Ä –ø–æ–ª–Ω–æ—Å—Ç—å—é, –Ω–∞—á–∏–Ω–∞—è —Å +7". 
–°–¥–µ–ª–∞–π –ø–µ—Ä–µ—Å–ø—Ä–æ—Å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑. 
–ü–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∫–ª–∏–µ–Ω—Ç–∞ –ù–ï –≤—ã–∑—ã–≤–∞–π —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–∏—à–∏ –Ω–æ–º–µ—Ä —Ç–∞–∫, –∫–∞–∫ –æ–Ω –±—ã–ª –ø—Ä–æ–¥–∏–∫—Ç–æ–≤–∞–Ω, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π. 
–ó–∞—Ç–µ–º –ø–µ—Ä–µ—Ö–æ–¥–∏ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –≤–æ–ø—Ä–æ—Å—É.
6.	–ê–¥—Ä–µ—Å –≤—ã–µ–∑–¥–∞ –º–∞—Å—Ç–µ—Ä–∞, –≤–∫–ª—é—á–∞—é—â–∏–π –º–∏–Ω–∏–º—É–º: –≥–æ—Ä–æ–¥, —É–ª–∏—Ü—É, –Ω–æ–º–µ—Ä –¥–æ–º–∞ (—Å –∫–æ—Ä–ø—É—Å–æ–º/—Å—Ç—Ä–æ–µ–Ω–∏–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å).
7.	–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∞–¥—Ä–µ—Å—É (–∫–≤–∞—Ä—Ç–∏—Ä–∞, –ø–æ–¥—ä–µ–∑–¥, —ç—Ç–∞–∂, –∫–æ–¥ –¥–æ–º–æ—Ñ–æ–Ω–∞) ‚Äî –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –∑–∞–ø—Ä–æ—Å–∏—Ç—å.
8. –î–∞—Ç–∞ –≤–∏–∑–∏—Ç–∞ –º–∞—Å—Ç–µ—Ä–∞ ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç —É–∂–µ —É–ø–æ–º—è–Ω—É–ª –¥–∞—Ç—É. –£—á–∏—Ç—ã–≤–∞–π, —á—Ç–æ —Å–µ–π—á–∞—Å 2025 –≥–æ–¥. 
9. –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ‚Äî –µ—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç —Ö–æ—á–µ—Ç –æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫–∏–µ-—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.
–í–µ–¥–∏ –¥–∏–∞–ª–æ–≥ –∞–∫—Ç–∏–≤–Ω–æ –∏ –≤–µ–∂–ª–∏–≤–æ, –∑–∞–¥–∞–≤–∞–π –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ –Ω–∞–∑–≤–∞–Ω—ã. –ù–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–π —Ä–∞–∑–≥–æ–≤–æ—Ä, –ø–æ–∫–∞ –Ω–µ —Å–æ–±–µ—Ä—ë—à—å –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è.
–ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ —Å–æ–±–µ—Ä—ë—à—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ, –≤—ã–∑–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏—é save_client_info —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞—è–≤–∫–∏.
–ü–æ–¥—Ç–≤–µ—Ä–¥–∏ –∫–ª–∏–µ–Ω—Ç—É, —á—Ç–æ –∑–∞—è–≤–∫–∞ –ø—Ä–∏–Ω—è—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞, –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–º–æ—â—å.
` ,
        turn_detection: {
          type: 'server_vad', // –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ —Å–µ—Ä–≤–µ—Ä–∞
          threshold: VAD_THRESHOLD,
          prefix_padding_ms: VAD_PREFIX_PADDING_MS,
          silence_duration_ms: VAD_SILENCE_DURATION_MS,
          create_response: true
        },
        input_audio_transcription: { model: 'whisper-1',
          language: 'ru'
         }, // –ú–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        "input_audio_noise_reduction" : {type: 'near_field'},
        "temperature": 0.6,
        //"max_response_output_tokens": 500,
        tools,
    tool_choice: 'auto'
      }
    }));
    logClient(`Session updated with VAD settings for channel ${channelId} | Threshold: ${VAD_THRESHOLD}, Prefix: ${VAD_PREFIX_PADDING_MS}ms, Silence: ${VAD_SILENCE_DURATION_MS}ms`);

    // Set max call duration timeout
    maxCallTimeoutId = setTimeout(async () => {
      logClient(`Max call duration (${MAX_CALL_DURATION}ms) reached for channel ${channelId}, closing connection and hanging up`);
      ws.close();
      const channelData = sipMap.get(channelId);
      if (channelData && channelData.bridge) {
        try {
          await ariClient.channels.hangup({ channelId: channelId });
          logger.info(`Channel ${channelId} hung up due to max call duration`);
        } catch (err) {
          logger.error(`Failed to hang up channel ${channelId}: ${err.message}`);
        }
      }
    }, MAX_CALL_DURATION);
  });

  

  ws.on('message', async (data) => {
    try {
      const response = JSON.parse(data.toString());
      receivedEventCounter++;


      if (response.type === 'response.done') {
        const outputs = response.response?.output || [];
        for (const output of outputs) {
          if (output.type === 'function_call' && output.name === 'save_client_info') {
            await handleSaveClientInfo(output, ws, logger);
          }
          if (output.type === 'function_call' && output.name === 'validate_phone') {
            await handleValidatePhone(output, ws, logger);
          }
        }
      }

    const duration = audioSentTime ? ((Date.now() - audioSentTime) / 1000).toFixed(2) : 'N/A';

    if (receivedEventCounter === 0) {
      logServer(`First event received for channel ${channelId} | Type: ${response.type} | Duration: ${duration}s | Status: Received`);
    }

    switch (response.type) {
      case 'session.created':
        logServer(`Session created for channel ${channelId} | Duration: ${duration}s | Status: Received`);
        break;
      case 'input_audio_buffer.speech_started':
        logServer(`Speech started detected for channel ${channelId} | Duration: ${duration}s | Status: Received`);
        break;
      case 'input_audio_buffer.speech_stopped':
        logServer(`Speech stopped detected for channel ${channelId} | Duration: ${duration}s | Status: Received`);
        audioSentTime = Date.now();
        if (streamHandler) {
          await streamHandler.stop();
          logger.info(`Stopped RTP stream due to user speech for channel ${channelId}`);
          streamHandler = null;
          await initializeStreamHandler();
          isPlayingResponse = false;
        }
        break;
      case 'conversation.item.input_audio_transcription.completed':
        logServer(`Sent audio transcribed for channel ${channelId} | Transcript: "${response.transcript.trim()}" | Duration: ${duration}s | Status: Received`);
        break;
      case 'response.audio.delta':
        audioDeltaCount++;
        if (!audioReceivedLogged) {
          responseTimestamp = Date.now();
          logServer(`Audio reception started for channel ${channelId} | Duration: ${duration}s | Status: Received`);
          audioReceivedLogged = true;
        }
        isPlayingResponse = true;
        const pcmChunk = Buffer.from(response.delta, 'base64');
        logServer(`Audio delta received for channel ${channelId} | Size: ${(pcmChunk.length / 1024).toFixed(2)} KB`);
        if (streamHandler) {
          streamHandler.write(pcmChunk);
          totalPacketsSentThisResponse += pcmChunk.length / 160;
          totalPacketsSentSession += pcmChunk.length / 160;
        } else {
          logger.error(`Failed to write audio delta: No StreamHandler for channel ${channelId}`);
        }
        break;
      case 'response.audio_transcript.delta':
        transcriptDeltaCount++;
        responseTranscript += response.delta;
        break;
      case 'response.audio_transcript.done':
        logServer(`Response received for channel ${channelId} | Transcript: "${response.transcript.trim()}" | Duration: ${duration}s | Status: Received`);
        responseTranscript = '';
        break;
      case 'response.done':
        audioReceivedLogged = false;
        isPlayingResponse = false;
        const stats = rtpSentStats.get(channelId) || { packets: 0, bytes: 0, startTime: responseTimestamp };
        const responseDuration = responseTimestamp ? ((Date.now() - responseTimestamp) / 1000).toFixed(2) : 'N/A';
        logServer(`Response completed for channel ${channelId} | Duration: ${responseDuration}s | Audio Fragments: ${audioDeltaCount} | Text Fragments: ${transcriptDeltaCount} | RTP Packets: ${stats.packets} | RTP Bytes: ${stats.bytes}`);
        audioDeltaCount = 0;
        transcriptDeltaCount = 0;
        totalPacketsSentThisResponse = 0;
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: 'input_audio_buffer.clear' }));
          logClient(`Cleared OpenAI audio buffer for channel ${channelId}`);
        }
        break;
      case 'error':
        logServer(`Error received for channel ${channelId} | Message: ${response.error.message} | Code: ${response.error.code || 'N/A'} | Status: Error`);
        if (streamHandler) {
          await streamHandler.stop();
          streamHandler = null;
        }
        break;
    }
  } catch (error) {
    logger.error(`–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: ${error.message}`);
  }
});

  ws.on('error', (error) => {
    logClient(`OpenAI WebSocket error for channel ${channelId} | Message: ${error.message} | Status: Error`);
    if (streamHandler) {
      streamHandler.stop();
      streamHandler = null;
    }
  });

  ws.on('close', () => {
    if (maxCallTimeoutId) clearTimeout(maxCallTimeoutId);
    if (streamHandler) {
      streamHandler.stop();
      streamHandler = null;
    }
    logClient(`OpenAI WebSocket connection closed for channel ${channelId} | Status: Finished`);
  });

  return { ws, getPlaybackComplete: () => playbackComplete, stopStream: () => streamHandler && streamHandler.stop() };
}

// Main async function to initialize ARI and handle events
(async () => {
  try {
    ariClient = await ari.connect(ARI_URL, ARI_USER, ARI_PASS); // Connect to ARI
    logger.info(`Connected to ARI at ${ARI_URL}`);
    await ariClient.start(ARI_APP); // Start Stasis app
    logger.info(`ARI application "${ARI_APP}" started`);

    startRTPReceiver(); // Start RTP receiver

    // Handle new channel entering Stasis
    ariClient.on('StasisStart', async (evt, channel) => {
      logger.info(`StasisStart event received for channel ${channel.id}, name: ${channel.name}`);
      const callerNumber = channel.caller && channel.caller.number ? channel.caller.number : null;
      logger.info(`üîî [PHONE] Caller number (phone2): ${callerNumber}`);
      // logger.info(JSON.stringify(channel, null, 2));
      if (channel.name && channel.name.startsWith('UnicastRTP')) { // ExternalMedia channel
        logger.info(`ExternalMedia channel started: ${channel.id}`);
        let mapping = extMap.get(channel.id);
        if (!mapping) {
          await new Promise(r => setTimeout(r, 500)); // Wait for mapping
          mapping = extMap.get(channel.id);
        }
        if (mapping) {
          await addExtToBridge(ariClient, channel, mapping.bridgeId);
          const channelData = sipMap.get(mapping.channelId);
          if (channelData && !channelData.rtpSource) {
            rtpReceiver.once('message', (msg, rinfo) => {
              channelData.rtpSource = rinfo; // Assign RTP source
              logger.info(`RTP Source assigned for channel ${mapping.channelId}: ${rinfo.address}:${rinfo.port}`);
            });
          }
        }
        return;
      }
      logger.info(`SIP channel started: ${channel.id}`);
      try {
        const bridge = await ariClient.bridges.create({ type: 'mixing,proxy_media' }); // Create mixing bridge
        await bridge.addChannel({ channel: channel.id });

        await channel.answer(); // Answer the call
        logger.info(`Channel ${channel.id} answered`);

        // Set up ExternalMedia channel
        const extParams = {
          app: ARI_APP,
          external_host: `127.0.0.1:${RTP_PORT}`,
          format: 'ulaw',
          transport: 'udp',
          encapsulation: 'rtp',
          connection_type: 'client',
          direction: 'both'
        };
        const extChannel = await ariClient.channels.externalMedia(extParams);
        extMap.set(extChannel.id, { bridgeId: bridge.id, channelId: channel.id });
        logger.info(`ExternalMedia channel ${extChannel.id} created and mapped to bridge ${bridge.id}`);

        const { ws, getPlaybackComplete, stopStream } = startOpenAIWebSocket(channel.id);
        sipMap.set(channel.id, { bridge, ws, channelId: channel.id, callerNumber, sendTimeout: null, getPlaybackComplete, stopStream });
      } catch (e) {
        logger.error(`Error in SIP channel ${channel.id}: ${e.message}`);
      }
    });

    // Handle channel leaving Stasis (call end)
    ariClient.on('StasisEnd', async (evt, channel) => {
      if (channel.name && channel.name.startsWith('UnicastRTP')) {
        extMap.delete(channel.id);
        logger.info(`ExternalMedia channel ${channel.id} removed from map`);
      } else {
        const channelData = sipMap.get(channel.id);
        if (channelData) {
          try {
            sipMap.delete(channel.id);
            logger.info(`Channel ${channel.id} removed from sipMap at start of StasisEnd`);

            if (channelData.sendTimeout) {
              clearInterval(channelData.sendTimeout);
              channelData.sendTimeout = null;
              logger.info(`Send timeout cleared for channel ${channel.id}`);
            }

            if (channelData.stopStream) {
              await channelData.stopStream();
              logger.info(`StreamHandler stopped for channel ${channel.id} in StasisEnd`);
            }

            if (!channelData.getPlaybackComplete()) {
              logger.info(`Channel ${channel.id} hung up, checking playback status before cleanup`);
              await new Promise(resolve => setTimeout(resolve, 100)); // Brief delay
            }

            if (channelData.ws && channelData.ws.readyState === WebSocket.OPEN) {
              channelData.ws.close();
              logger.info(`WebSocket closed for channel ${channel.id} in StasisEnd`);
            }

            await channelData.bridge.destroy();
            logger.info(`Bridge ${channelData.bridge.id} destroyed`);
          } catch (e) {
            logger.error(`Error during cleanup for channel ${channel.id}: ${e.message}`);
          }
        }
        logger.info(`Channel ended: ${channel.id}`);

        // Save audio files if enabled
        if (ENABLE_SENT_TO_OPENAI_RECORDING && audioFromAsteriskMap.has(channel.id) && audioFromAsteriskMap.get(channel.id).length > 0) {
          saveRawFile(audioFromAsteriskMap.get(channel.id), `asterisk_input_mulaw_raw_${channel.id}.raw`);
          audioFromAsteriskMap.delete(channel.id);
        }
        if (ENABLE_SENT_TO_OPENAI_RECORDING && audioToOpenAIMap.has(channel.id) && audioToOpenAIMap.get(channel.id).length > 0) {
          saveWavFile(audioToOpenAIMap.get(channel.id), `sent_to_openai_${channel.id}.wav`, 24000);
          audioToOpenAIMap.delete(channel.id);
        }
      }
    });

    ariClient.on('error', (err) => logger.error(`ARI client error: ${err.message}`));
    ariClient.on('close', () => logger.info('ARI WebSocket connection closed'));
  } catch (err) {
    logger.error(`ARI connection error: ${err.message}`);
    process.exit(1); // Exit on connection failure
  }
})();

// Handle uncaught exceptions
process.on('uncaughtException', (err) => {
  logger.error(`Uncaught Exception: ${err.message}`);
  cleanup();
  process.exit(1);
});

// Handle SIGINT (Ctrl+C)
process.on('SIGINT', () => {
  logger.info('Received SIGINT, cleaning up...');
  cleanup();
  process.exit(0);
});

// Cleanup function to close sockets and connections
function cleanup() {
  sipMap.forEach((data, channelId) => {
    if (data.ws) data.ws.close(); // Close WebSocket
    if (data.sendTimeout) clearInterval(data.sendTimeout); // Clear send interval
    if (data.stopStream) data.stopStream(); // Stop RTP stream
  });
  rtpSender.close(); // Close RTP sender socket
  rtpReceiver.close(); // Close RTP receiver socket
}
